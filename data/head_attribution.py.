#!/usr/bin/env python3
"""
head_attribution.py
====================

Compute attention-head attribution scores for a target token using
gradient × activation style attribution.

This script estimates which attention heads most influence the model’s
prediction for a given token in context. It can use either:

1. **TransformerLens** (preferred, if installed)
2. **Hugging Face Transformers** fallback (GPT-2 family or compatible models)

Output:
--------
A JSON file with normalized per-head attribution scores.

Example:
---------
    python head_attribution.py \
        --model gpt2 \
        --text "The capital of France is Paris." \
        --target_token "Paris" \
        --output results/head_attribution.json

References:
------------
- Voita et al., “Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting”, ACL 2019
- Jain & Wallace, “Attention is not Explanation”, NAACL 2019
- Vig, “A Multiscale Visualization of Attention in the Transformer Model”, 2019
"""

import argparse
import json
import torch
from collections import defaultdict
from typing import Dict, Any

# -------------------------
# Utilities
# -------------------------
def save_json(path: str, data: Dict[str, Any]) -> None:
    """Save a Python dictionary as UTF-8 JSON with indentation."""
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)


def try_import_transformer_lens():
    """Try importing TransformerLens."""
    try:
        from transformer_lens import HookedTransformer
        return HookedTransformer
    except Exception:
        return None


# -------------------------
# TransformerLens pipeline
# -------------------------
def compute_with_transformer_lens(model_name, text, target_token_str, device):
    from transformer_lens import HookedTransformer

    model = HookedTransformer.from_pretrained(model_name, device=device)
    model.eval()

    toks = model.to_tokens(text).to(device)
    tokens_text = model.to_str_tokens(toks[0])

    # locate target token(s)
    target_idxs = [i for i, tok in enumerate(tokens_text) if target_token_str in tok]
    if not target_idxs:
        raise ValueError(f"Target token '{target_token_str}' not found in input: {tokens_text}")

    # forward pass with cache
    logits, cache = model.run_with_cache(toks)

    # pick target position (last occurrence)
    target_pos = target_idxs[-1]

    # gradient of max logit at target position
    model.zero_grad()
    logit = logits[0, target_pos, :].max()
    logit.backward(retain_graph=True)

    # gather attention activations and grads
    attributions = defaultdict(list)
    for key, value in cache.items():
        if any(k in key for k in ["attn", "hook_z", "hook_v", "attn_out"]) and isinstance(value, torch.Tensor):
            v = value.detach()
            grad = value.grad
            if grad is None:
                score = v.norm().item()
            else:
                score = (grad * v).abs().mean().item()
            attributions[key].append(score)

    # average per key
    return {k: float(sum(v) / len(v)) for k, v in attributions.items() if v}


# -------------------------
# Hugging Face fallback
# -------------------------
def compute_with_hf_gpt2(model_name, text, target_token_str, device):
    from transformers import AutoModelForCausalLM, AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
    model = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True).to(device)
    model.eval()

    inputs = tokenizer(text, return_tensors="pt").to(device)
    input_ids = inputs["input_ids"]

    # find target token id(s)
    target_ids = tokenizer(target_token_str, add_special_tokens=False)["input_ids"]
    seq = input_ids[0].tolist()

    # find position
    target_pos = None
    for i in range(len(seq) - len(target_ids) + 1):
        if seq[i:i + len(target_ids)] == target_ids:
            target_pos = i + len(target_ids) - 1
            break
    if target_pos is None:
        raise ValueError(f"Target token '{target_token_str}' not found in tokenized sequence.")

    # forward + grad
    model.zero_grad()
    outputs = model(**inputs)
    logits = outputs.logits
    logit = logits[0, target_pos, seq[target_pos]]
    logit.backward(retain_graph=True)

    # hook attention outputs
    head_acts, head_grads = {}, {}
    hooks = []

    def make_hook(name):
        def hook(module, inp, out):
            head_acts[name] = out.detach().clone().requires_grad_(True)
            def grad_fn(grad):
                head_grads[name] = grad.detach().clone()
            head_acts[name].register_hook(grad_fn)
        return hook

    for name, module in model.named_modules():
        if name.endswith("attn") or ("attn" in name and "mlp" not in name):
            try:
                hooks.append(module.register_forward_hook(make_hook(name)))
            except Exception:
                pass

    model.zero_grad()
    out2 = model(**inputs)
    logits2 = out2.logits
    logit2 = logits2[0, target_pos, seq[target_pos]]
    logit2.backward()

    for h in hooks:
        h.remove()

    attributions = {}
    for name, act in head_acts.items():
        grad = head_grads.get(name)
        score = (act.abs().mean().item() if grad is None else (act * grad).abs().mean().item())
        attributions[name] = score

    return attributions


# -------------------------
# Main CLI
# -------------------------
def main():
    parser = argparse.ArgumentParser(description="Compute attention head attribution via grad×activation.")
    parser.add_argument("--model", type=str, default="gpt2", help="Model name (transformers or TransformerLens ID)")
    parser.add_argument("--text", type=str, required=True, help="Input text containing the target token")
    parser.add_argument("--target_token", type=str, required=True, help="Substring or token to attribute (e.g., 'Paris')")
    parser.add_argument("--device", type=str, default="cpu", help="Device: 'cpu' or 'cuda'")
    parser.add_argument("--output", type=str, default="head_attribution_results.json", help="Output JSON path")
    args = parser.parse_args()

    device = torch.device("cuda" if torch.cuda.is_available() and "cuda" in args.device else "cpu")
    print(f"[INFO] Using device: {device}")

    HT = try_import_transformer_lens()
    if HT is not None:
        print("[INFO] TransformerLens detected. Using HookedTransformer pipeline.")
        try:
            attributions = compute_with_transformer_lens(args.model, args.text, args.target_token, device)
        except Exception as e:
            print(f"[WARN] TransformerLens path failed: {e}")
            print("[INFO] Falling back to HuggingFace pipeline.")
            attributions = compute_with_hf_gpt2(args.model, args.text, args.target_token, device)
    else:
        print("[INFO] TransformerLens not available. Using HuggingFace pipeline.")
        attributions = compute_with_hf_gpt2(args.model, args.text, args.target_token, device)

    # Normalize to [0,1]
    vals = list(attributions.values())
    if vals:
        minv, maxv = min(vals), max(vals)
    else:
        minv, maxv = 0.0, 1.0
    normalized = {
        k: {"raw": v, "norm": (v - minv) / (maxv - minv + 1e-12)} for k, v in attributions.items()
    }

    result = {
        "model": args.model,
        "text": args.text,
        "target_token": args.target_token,
        "device": str(device),
        "attributions": normalized,
    }

    save_json(args.output, result)
    print(f"[OK] Saved attribution results to {args.output}")


if __name__ == "__main__":
    main()
